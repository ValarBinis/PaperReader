"""
AutoAgent - è‡ªåŠ¨åŒ–æµç¨‹Agent
ä¸€é”®å¼å®Œæˆï¼šæœç´¢â†’ä¸‹è½½â†’åˆ†æâ†’æ‰©å……â†’å›¾è°±â†’è¾“å‡º
"""

import hashlib
from typing import List, Dict, Any, Optional
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime

from .query_expander import QueryExpander
from .reference_miner import ReferenceMiner
from ..apis.llm_client import LLMClient
from ..apis.arxiv_api import ArxivAPI, ArxivPaper
from ..apis.scihub_api import SciHubAPI, SciHubPaper
from ..apis.paper_base import BasePaper, BasePaperAPI, PaperSource, get_paper_source
from ..analyzers.paper_analyzer import PaperAnalyzer, PaperAnalysis
from ..graph.citation_graph import CitationGraph
from ..graph.obsidian_renderer import ObsidianRenderer
from ..utils.config import get_config
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn


@dataclass
class AgentConfig:
    """Agenté…ç½®"""
    max_papers: int = 20
    download_pdfs: bool = True
    analyze_full_text: bool = True
    expand_references: bool = False
    ref_max_depth: int = 1
    build_graph: bool = True
    output_obsidian: bool = True
    output_index: bool = True
    paper_sources: List[str] = None  # è®ºæ–‡æ•°æ®æºåˆ—è¡¨ ['arxiv', 'scihub']

    def __post_init__(self):
        if self.paper_sources is None:
            self.paper_sources = ['arxiv']  # é»˜è®¤åªä½¿ç”¨arxiv


class AutoAgent:
    """
    è‡ªåŠ¨åŒ–è®ºæ–‡å¤„ç†Agent
    ä¸€é”®å¼å®Œæˆä»æœç´¢åˆ°è¾“å‡ºçš„å®Œæ•´æµç¨‹
    """

    def __init__(
        self,
        llm_client: LLMClient,
        config: AgentConfig = None,
        output_topic: str = None,
        search_field: str = "all"
    ):
        """
        åˆå§‹åŒ–AutoAgent

        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            config: Agenté…ç½®
            output_topic: è¾“å‡ºä¸»é¢˜ï¼ˆç”¨äºåˆ›å»ºå…³é”®è¯ç‰¹å®šçš„è¾“å‡ºæ–‡ä»¶å¤¹ï¼‰
            search_field: æœç´¢å­—æ®µï¼ˆall/ti/absï¼‰
        """
        self.llm_client = llm_client
        self.config = config or AgentConfig()
        self.output_topic = output_topic or "default"
        self.search_field = search_field
        self.console = Console()

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.query_expander = QueryExpander(llm_client)
        self.reference_miner = None  # å°†åœ¨åˆå§‹åŒ–paper_apisåè®¾ç½®

        # åˆå§‹åŒ–å¤šä¸ªè®ºæ–‡æ•°æ®æºAPI
        self.paper_apis: Dict[str, BasePaperAPI] = {}
        self._init_paper_apis()

        # åˆå§‹åŒ–åˆ†æå™¨
        self.analyzer = PaperAnalyzer(
            llm_client=llm_client,
            analyze_full_text=self.config.analyze_full_text
        )

        # ç»“æœå­˜å‚¨
        self.papers: List[BasePaper] = []
        self.analyses: List[PaperAnalysis] = []
        self.graph: Optional[CitationGraph] = None

    def _init_paper_apis(self):
        """åˆå§‹åŒ–è®ºæ–‡æ•°æ®æºAPI"""
        config = get_config()

        # åˆå§‹åŒ–arXiv API
        if 'arxiv' in self.config.paper_sources:
            search_config = config.search
            self.paper_apis['arxiv'] = ArxivAPI(
                max_results=self.config.max_papers,
                sort_by=search_config.get("sort_by", "relevance"),
                categories=search_config.get("categories", []),
                search_field=self.search_field
            )

        # åˆå§‹åŒ–Sci-Hub API
        if 'scihub' in self.config.paper_sources:
            scihub_config = config.get("scihub", {})
            if scihub_config.get("enabled", True):
                self.paper_apis['scihub'] = SciHubAPI(
                    base_url=scihub_config.get("base_url", ""),
                    timeout=scihub_config.get("timeout", 60),
                    max_retries=scihub_config.get("max_retries", 3)
                )

        # åˆå§‹åŒ–ReferenceMinerï¼ˆä½¿ç”¨arXiv APIï¼‰
        if 'arxiv' in self.paper_apis:
            self.reference_miner = ReferenceMiner(self.paper_apis['arxiv'])

    def run(self, topic: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„è‡ªåŠ¨åŒ–æµç¨‹

        Args:
            topic: ç ”ç©¶ä¸»é¢˜

        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        self.console.print(Panel.fit(
            f"[bold cyan]ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–å¤„ç†: {topic}[/bold cyan]"
        ))

        result = {
            "topic": topic,
            "start_time": datetime.now().isoformat(),
            "papers_found": 0,
            "papers_analyzed": 0,
            "output_files": []
        }

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=self.console
        ) as progress:

            # æ­¥éª¤1-2: AIæ‹“å±•æœç´¢è¯å¹¶æœç´¢arXivè®ºæ–‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            max_retries = 5
            retry_count = 0
            self.papers = []
            failed_keywords_history = []  # è®°å½•å¤±è´¥çš„å…³é”®è¯

            while retry_count < max_retries and not self.papers:
                retry_count += 1

                # æ­¥éª¤1: AIæ‹“å±•æœç´¢è¯
                task = progress.add_task(f"[cyan]AIæ‹“å±•æœç´¢è¯ (å°è¯• {retry_count}/{max_retries})...", total=1)

                # é¦–æ¬¡ä½¿ç”¨åŸå§‹ä¸»é¢˜ï¼Œåç»­ä¼ å…¥å¤±è´¥çš„å…³é”®è¯å†å²
                if retry_count == 1:
                    search_topic = topic
                    expanded_queries = self.query_expander.expand_query(search_topic)
                else:
                    # å°†å¤±è´¥çš„å…³é”®è¯å†å²ä¼ é€’ç»™LLM
                    expanded_queries = self.query_expander.expand_query_with_feedback(
                        topic,
                        failed_keywords_history
                    )

                progress.update(task, completed=1)
                self.console.print(f"[green]âœ“[/green] ç”Ÿæˆ {len(expanded_queries)} ä¸ªæœç´¢è¯")

                # æ­¥éª¤2: æœç´¢arXivè®ºæ–‡
                task = progress.add_task(f"[cyan]æœç´¢arXivè®ºæ–‡ (å°è¯• {retry_count}/{max_retries})...", total=len(expanded_queries))
                self.papers = self._search_papers(expanded_queries, progress, task)

                if self.papers:
                    result["papers_found"] = len(self.papers)
                    self.console.print(f"[green]âœ“[/green] æ‰¾åˆ° {len(self.papers)} ç¯‡è®ºæ–‡")
                elif retry_count < max_retries:
                    # è®°å½•å¤±è´¥çš„å…³é”®è¯
                    failed_keywords_history.extend(expanded_queries)
                    self.console.print(f"[yellow]æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œæ­£åœ¨é‡æ–°ç”Ÿæˆå…³é”®è¯... ({retry_count}/{max_retries})[/yellow]")

            if not self.papers:
                self.console.print(f"[red]å·²å°è¯• {max_retries} æ¬¡ï¼Œä»æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡[/red]")
                self.console.print(f"[dim]ä½¿ç”¨è¿‡çš„å…³é”®è¯: {failed_keywords_history}[/dim]")
                self.console.print("[yellow]å»ºè®®ï¼šå°è¯•æ›´æ¢æ›´é€šç”¨çš„æœç´¢è¯æˆ–æ£€æŸ¥ä¸»é¢˜æ˜¯å¦è¿‡äºåé—¨[/yellow]")
                return result

            # æ­¥éª¤3: ä¸‹è½½PDF
            pdf_paths = {}
            if self.config.download_pdfs:
                task = progress.add_task("[cyan]ä¸‹è½½PDF...", total=len(self.papers))
                pdf_paths = self._download_pdfs(self.papers, progress, task)

            # æ­¥éª¤4: åˆ†æè®ºæ–‡
            task = progress.add_task("[cyan]åˆ†æè®ºæ–‡...", total=len(self.papers))
            self.analyses = self._analyze_papers(self.papers, pdf_paths, progress, task)
            result["papers_analyzed"] = len(self.analyses)
            self.console.print(f"[green]âœ“[/green] åˆ†æå®Œæˆ {len(self.analyses)} ç¯‡")

            # æ­¥éª¤5: æ„å»ºçŸ¥è¯†å›¾è°±
            if self.config.build_graph:
                task = progress.add_task("[cyan]æ„å»ºçŸ¥è¯†å›¾è°±...", total=1)
                self.graph = self._build_graph(self.analyses)
                progress.update(task, completed=1)
                self.console.print(f"[green]âœ“[/green] çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")

            # æ­¥éª¤6: å‚è€ƒæ–‡çŒ®æ‰©å……ï¼ˆæ”¯æŒCtrl+Cä¸­æ–­ï¼‰
            if self.config.expand_references:
                # ä¼°ç®—æ€»ä»»åŠ¡æ•°ï¼ˆæ¯å±‚è®ºæ–‡æ•° Ã— å±‚æ•°ï¼‰
                estimated_total = len(self.papers) * self.config.ref_max_depth
                task = progress.add_task("[cyan]æŒ–æ˜å‚è€ƒæ–‡çŒ®...", total=estimated_total)

                try:
                    expanded_papers = self._expand_references(self.papers, progress, task)
                    if expanded_papers:
                        self.papers.extend(expanded_papers)
                        self.console.print(f"[green]âœ“[/green] æ‰©å…… {len(expanded_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
                except KeyboardInterrupt:
                    # ç”¨æˆ·ä¸­æ–­ï¼Œç»§ç»­åç»­æ­¥éª¤
                    self.console.print("[yellow]å‚è€ƒæ–‡çŒ®æŒ–æ˜å·²ä¸­æ–­ï¼Œç»§ç»­åç»­æ­¥éª¤...[/yellow]")
                    progress.update(task, completed=estimated_total)

            # æ­¥éª¤7: ç”Ÿæˆè¾“å‡º
            task = progress.add_task("[cyan]ç”Ÿæˆè¾“å‡º...", total=3)
            output_files = self._generate_outputs(topic)
            result["output_files"] = output_files
            progress.update(task, completed=3)

        result["end_time"] = datetime.now().isoformat()

        self.console.print(Panel.fit(
            f"[bold green]âœ… å¤„ç†å®Œæˆï¼[/bold green]\n"
            f"è®ºæ–‡æ•°: {len(self.analyses)}\n"
            f"è¾“å‡ºæ–‡ä»¶: {len(output_files)} ä¸ª"
        ))

        return result

    def _search_papers(
        self,
        queries: List[str],
        progress,
        task: int
    ) -> List[BasePaper]:
        """
        ä½¿ç”¨å¤šä¸ªæœç´¢è¯æœç´¢è®ºæ–‡ï¼ˆæ”¯æŒå¤šæ•°æ®æºï¼‰

        Args:
            queries: æœç´¢è¯åˆ—è¡¨
            progress: Progresså¯¹è±¡
            task: ä»»åŠ¡ID

        Returns:
            è®ºæ–‡åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        """
        all_papers = []
        seen_ids = set()

        # è®¡ç®—æ¯ä¸ªæ•°æ®æºåº”è¯¥è·å–çš„è®ºæ–‡æ•°
        num_sources = len(self.paper_apis)
        if num_sources == 0:
            return []

        papers_per_source = max(5, self.config.max_papers // num_sources)

        # ä»æ¯ä¸ªæ•°æ®æºæœç´¢
        for source_name, api in self.paper_apis.items():
            for query in queries:
                try:
                    # æ ¹æ®æ•°æ®æºç±»å‹è°ƒæ•´æœç´¢å‚æ•°
                    if source_name == 'scihub':
                        # Sci-Hubä¸»è¦æ”¯æŒDOIæŸ¥è¯¢ï¼Œå¯¹äºæ™®é€šæŸ¥è¯¢ä½¿ç”¨CrossRef
                        papers = api.search(
                            query=query,
                            max_results=1,
                            use_crossref=True
                        )
                    else:
                        # arXivç­‰æ”¯æŒæ™®é€šæœç´¢
                        papers = api.search(
                            query=query,
                            max_results=papers_per_source
                        )

                    for paper in papers:
                        # ä½¿ç”¨paper_idä½œä¸ºå”¯ä¸€æ ‡è¯†
                        paper_id = paper.paper_id
                        if paper_id and paper_id not in seen_ids:
                            seen_ids.add(paper_id)
                            all_papers.append(paper)

                            if len(all_papers) >= self.config.max_papers:
                                break

                except Exception as e:
                    self.console.print(f"[yellow]{source_name}æœç´¢å¤±è´¥ ({query}): {e}[/yellow]")

                progress.update(task, advance=1)

                if len(all_papers) >= self.config.max_papers:
                    break

            if len(all_papers) >= self.config.max_papers:
                break

        return all_papers[:self.config.max_papers]

    def _download_pdfs(
        self,
        papers: List[BasePaper],
        progress,
        task: int
    ) -> Dict[str, str]:
        """
        ä¸‹è½½è®ºæ–‡PDFï¼ˆæ”¯æŒå¤šæ•°æ®æºï¼‰

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            progress: Progresså¯¹è±¡
            task: ä»»åŠ¡ID

        Returns:
            {paper_id: pdf_path} å­—å…¸
        """
        pdf_paths = {}
        papers_dir = Path(get_config().output.get("papers_dir", "./data/papers"))
        papers_dir.mkdir(parents=True, exist_ok=True)

        for paper in papers:
            try:
                from ..utils.io import safe_filename_from_title

                # ä½¿ç”¨paper_idä½œä¸ºæ–‡ä»¶åå‰ç¼€ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨æ ‡é¢˜çš„hash
                paper_id = paper.paper_id or hashlib.md5(paper.title.encode()).hexdigest()[:8]
                filename = f"{paper_id}_{safe_filename_from_title(paper.title)}.pdf"
                pdf_path = papers_dir / filename

                if not pdf_path.exists():
                    # æ ¹æ®è®ºæ–‡æ¥æºé€‰æ‹©å¯¹åº”çš„APIä¸‹è½½
                    paper_source = get_paper_source(paper)
                    if paper_source == PaperSource.SCIHUB and 'scihub' in self.paper_apis:
                        self.paper_apis['scihub'].download_pdf(paper, str(pdf_path))
                    elif paper_source == PaperSource.ARXIV and 'arxiv' in self.paper_apis:
                        self.paper_apis['arxiv'].download_pdf(paper, str(pdf_path))
                    else:
                        # å°è¯•ä½¿ç”¨é€šç”¨ä¸‹è½½æ–¹æ³•
                        if paper.pdf_url:
                            self._download_pdf_direct(paper.pdf_url, str(pdf_path))

                pdf_paths[paper_id] = str(pdf_path)

            except Exception as e:
                self.console.print(f"[yellow]ä¸‹è½½å¤±è´¥ ({paper.title[:30]}...): {e}[/yellow]")

            progress.update(task, advance=1)

        return pdf_paths

    def _download_pdf_direct(self, pdf_url: str, save_path: str, timeout: int = 120) -> bool:
        """
        ç›´æ¥ä¸‹è½½PDF

        Args:
            pdf_url: PDFé“¾æ¥
            save_path: ä¿å­˜è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            import requests
            response = requests.get(pdf_url, timeout=timeout, stream=True)
            response.raise_for_status()

            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            return True
        except Exception:
            return False

    def _analyze_papers(
        self,
        papers: List[BasePaper],
        pdf_paths: Dict[str, str],
        progress,
        task: int
    ) -> List[PaperAnalysis]:
        """
        åˆ†æè®ºæ–‡

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            pdf_paths: PDFè·¯å¾„å­—å…¸
            progress: Progresså¯¹è±¡
            task: ä»»åŠ¡ID

        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        analyses = []

        for paper in papers:
            try:
                paper_id = paper.paper_id or hashlib.md5(paper.title.encode()).hexdigest()[:8]
                pdf_path = pdf_paths.get(paper_id)

                # æ ¹æ®è®ºæ–‡ç±»å‹è°ƒç”¨ç›¸åº”çš„åˆ†ææ–¹æ³•
                paper_source = get_paper_source(paper)
                if paper_source == PaperSource.ARXIV:
                    analysis = self.analyzer.analyze_from_arxiv(paper, pdf_path)
                elif isinstance(paper, BasePaper):
                    # é€šç”¨åˆ†ææ–¹æ³•
                    analysis = self.analyzer.analyze_from_paper(paper, pdf_path)
                else:
                    # å…œåº•ï¼šä½¿ç”¨é€šç”¨åˆ†ææ–¹æ³•
                    analysis = self.analyzer.analyze_from_paper(paper, pdf_path)

                analyses.append(analysis)
            except Exception as e:
                self.console.print(f"[yellow]åˆ†æå¤±è´¥ ({paper.title[:30]}...): {e}[/yellow]")

            progress.update(task, advance=1)

        return analyses

    def _build_graph(self, analyses: List[PaperAnalysis]) -> CitationGraph:
        """
        æ„å»ºå¼•ç”¨å…³ç³»å›¾

        Args:
            analyses: åˆ†æç»“æœåˆ—è¡¨

        Returns:
            CitationGraphå¯¹è±¡
        """
        graph = CitationGraph()

        for analysis in analyses:
            graph.add_paper_from_analysis(analysis)

        return graph

    def _expand_references(
        self,
        papers: List[BasePaper],
        progress,
        task: int
    ) -> List[BasePaper]:
        """
        åŸºäºå‚è€ƒæ–‡çŒ®é€’å½’æ‰©å……è®ºæ–‡
        ä¼˜å…ˆä½¿ç”¨LLMåˆ†ææ—¶æå–çš„å‚è€ƒæ–‡çŒ®
        æ”¯æŒå¤šæ•°æ®æºï¼šarXiv + Sci-Hub

        Args:
            papers: å½“å‰è®ºæ–‡åˆ—è¡¨
            progress: Progresså¯¹è±¡
            task: ä»»åŠ¡ID

        Returns:
            æ–°å‘ç°çš„è®ºæ–‡åˆ—è¡¨
        """
        expanded = []
        seen_ids = set(p.paper_id for p in papers if p.paper_id)
        papers_dir = Path(get_config().output.get("papers_dir", "./data/papers"))
        max_depth = self.config.ref_max_depth
        total_found = 0

        # æ„å»ºè®ºæ–‡åˆ†æç»“æœç´¢å¼•ï¼ˆpaper_id -> analysisï¼‰
        analysis_map = {}
        for analysis in self.analyses:
            paper_id = analysis.paper_id or analysis.arxiv_id
            if paper_id:
                analysis_map[paper_id] = analysis

        # é€’å½’æŒ–æ˜å‚è€ƒæ–‡çŒ®
        def mine_recursive(current_papers: List[BasePaper], current_depth: int) -> None:
            nonlocal total_found
            if current_depth > max_depth:
                return

            depth_papers_found = 0
            total_to_process = len(current_papers)

            for i, paper in enumerate(current_papers):
                # æ›´æ–°è¿›åº¦æ˜¾ç¤º
                progress.update(
                    task,
                    description=f"[cyan]æŒ–æ˜å‚è€ƒæ–‡çŒ® (æ·±åº¦{current_depth}/{max_depth}, {i+1}/{total_to_process}, å·²å‘ç°{total_found}ç¯‡)..."
                )

                try:
                    found_papers = {}  # {title: paper_obj}
                    paper_id = paper.paper_id or hashlib.md5(paper.title.encode()).hexdigest()[:8]

                    # ä¼˜å…ˆä½¿ç”¨LLMåˆ†ææ—¶æå–çš„å‚è€ƒæ–‡çŒ®
                    reference_titles = []
                    if paper_id in analysis_map:
                        llm_refs = analysis_map[paper_id].references
                        if llm_refs:
                            self.console.print(f"[dim]  ä½¿ç”¨LLMæå–çš„ {len(llm_refs)} ç¯‡å‚è€ƒæ–‡çŒ®[/dim]")
                            reference_titles = llm_refs

                    # å¦‚æœLLMæå–å¤±è´¥æˆ–ä¸ºç©ºï¼Œå›é€€åˆ°PDFæå–
                    if not reference_titles and paper_id:
                        pdf_files = list(papers_dir.glob(f"{paper_id}_*.pdf"))
                        if pdf_files and 'arxiv' in self.paper_apis:
                            from .reference_miner import ReferenceMiner
                            miner = ReferenceMiner(self.paper_apis['arxiv'])
                            references = miner.extract_references(str(pdf_files[0]))
                            # ä»Referenceå¯¹è±¡ä¸­æå–æ ‡é¢˜
                            reference_titles = [ref.title for ref in references if ref.title]

                    # å¤šæ•°æ®æºæœç´¢å‚è€ƒæ–‡çŒ®
                    for title in reference_titles[:30]:  # é™åˆ¶å¤„ç†æ•°é‡
                        # 1. å…ˆå°è¯•arXiv
                        if 'arxiv' in self.paper_apis:
                            try:
                                results = self.paper_apis['arxiv'].search(
                                    query=f"ti:{title}",
                                    max_results=1,
                                    timeout=3
                                )
                                if results:
                                    new_paper = results[0]
                                    if new_paper.paper_id not in seen_ids:
                                        found_papers[title] = new_paper
                                        seen_ids.add(new_paper.paper_id)
                                        continue
                            except Exception:
                                pass

                        # 2. å¦‚æœarXivæ²¡æ‰¾åˆ°ï¼Œå°è¯•Sci-Hub/CrossRef
                        if title not in found_papers and 'scihub' in self.paper_apis:
                            try:
                                results = self.paper_apis['scihub'].search(
                                    query=title,
                                    max_results=1,
                                    use_crossref=True
                                )
                                if results:
                                    new_paper = results[0]
                                    if new_paper.paper_id and new_paper.paper_id not in seen_ids:
                                        found_papers[title] = new_paper
                                        seen_ids.add(new_paper.paper_id)
                            except Exception:
                                pass

                    # æ·»åŠ æ–°å‘ç°çš„è®ºæ–‡
                    for title, new_paper in found_papers.items():
                        expanded.append(new_paper)
                        total_found += 1
                        depth_papers_found += 1

                        # æ˜¾ç¤ºæ¥æº
                        source = get_paper_source(new_paper)
                        self.console.print(
                            f"[dim]  + [{current_depth}][{source.upper()}] {new_paper.title[:50]}...[/dim]"
                        )

                except KeyboardInterrupt:
                    # ç”¨æˆ·ä¸­æ–­ï¼Œè¿”å›å·²å‘ç°çš„è®ºæ–‡
                    self.console.print(f"\n[yellow]ç”¨æˆ·ä¸­æ–­å‚è€ƒæ–‡çŒ®æŒ–æ˜[/yellow]")
                    raise
                except Exception as e:
                    self.console.print(f"[dim]  æŒ–æ˜å¤±è´¥ ({paper.title[:30]}...): {e}[/dim]")

                progress.update(task, advance=1)

                # é™åˆ¶å•å±‚æ‰©å……æ•°é‡é¿å…è¿‡åº¦è†¨èƒ€
                if depth_papers_found >= 30:
                    break

            # å¦‚æœå‘ç°äº†æ–°è®ºæ–‡ä¸”æœªè¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œç»§ç»­é€’å½’
            if depth_papers_found > 0 and current_depth < max_depth:
                new_papers = expanded[-depth_papers_found:]
                mine_recursive(new_papers, current_depth + 1)

        try:
            mine_recursive(papers, 1)
        except KeyboardInterrupt:
            # ç”¨æˆ·ä¸­æ–­ï¼Œç»§ç»­åç»­æ­¥éª¤
            self.console.print(f"[yellow]å‚è€ƒæ–‡çŒ®æŒ–æ˜å·²åœæ­¢ï¼Œå…±å‘ç° {total_found} ç¯‡æ–°è®ºæ–‡[/yellow]")

        return expanded

    def _generate_outputs(self, topic: str) -> List[str]:
        """
        ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶

        Args:
            topic: ç ”ç©¶ä¸»é¢˜

        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        output_files = []
        config = get_config()

        # åˆ›å»ºå…³é”®è¯ç‰¹å®šçš„è¾“å‡ºæ–‡ä»¶å¤¹
        from ..utils.io import sanitize_filename
        safe_topic = sanitize_filename(topic)
        topic_output_dir = Path("./output") / safe_topic
        topic_output_dir.mkdir(parents=True, exist_ok=True)

        # ObsidiançŸ¥è¯†åº“
        if self.config.output_obsidian:
            vault_path = topic_output_dir / "vault"
            renderer = ObsidianRenderer(str(vault_path))

            # å¯¼å‡ºæ‰€æœ‰è®ºæ–‡ç¬”è®°
            paper_files = renderer.export_all_papers(self.analyses)
            output_files.extend(paper_files)

            # ç”Ÿæˆä¸»é¢˜ç´¢å¼•
            topic_file = renderer.save_topic_index(
                topic,
                self.analyses,
                description=f"å…³äº{topic}çš„ç ”ç©¶è®ºæ–‡é›†åˆ"
            )
            output_files.append(topic_file)

            # ç”ŸæˆMOC
            moc_file = renderer.save_moc(self.analyses, self.graph)
            output_files.append(moc_file)

        # ç´¢å¼•æ–‡ä»¶
        if self.config.output_index:
            from ..renderers.markdown_gen import MarkdownGenerator

            generator = MarkdownGenerator()
            index_file = topic_output_dir / "papers_index.md"
            content = generator.generate_index(self.analyses, title=f"{topic} - è®ºæ–‡ç´¢å¼•")

            index_file.parent.mkdir(parents=True, exist_ok=True)
            with open(index_file, "w", encoding="utf-8") as f:
                f.write(content)

            output_files.append(str(index_file))

        # å¯¼å‡ºå›¾è°±
        if self.graph:
            graph_dir = topic_output_dir / "graph"
            graph_dir.mkdir(parents=True, exist_ok=True)

            json_file = graph_dir / "citation_graph.json"
            self.graph.export_json(str(json_file))
            output_files.append(str(json_file))

            graphml_file = graph_dir / "citation_graph.graphml"
            self.graph.export_graphml(str(graphml_file))
            output_files.append(str(graphml_file))

        return output_files


def run_auto_agent(topic: str, llm_client: LLMClient, **kwargs) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè¿è¡ŒAutoAgent

    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        llm_client: LLMå®¢æˆ·ç«¯
        **kwargs: Agenté…ç½®å‚æ•°

    Returns:
        æ‰§è¡Œç»“æœå­—å…¸
    """
    config = AgentConfig(**kwargs)
    agent = AutoAgent(llm_client, config, output_topic=topic)
    return agent.run(topic)
