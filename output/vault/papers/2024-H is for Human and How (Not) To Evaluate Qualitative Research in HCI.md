---
tags:
  - paper
arxiv_id: 2409.01302v2
published: 2024-08-20
authors: Andy Crabtree
category: 
---

# H is for Human and How (Not) To Evaluate Qualitative Research in HCI

## 基本信息
- **arXiv ID:** [2409.01302v2](http://arxiv.org/abs/2409.01302v2)
- **作者:** [[Andy Crabtree]]
- **发布时间:** 2024-08-20
- **分类:** 

<details>
<summary>详细摘要</summary>

## 论文信息
- **标题：** H is for Human and How (Not) To Evaluate Qualitative Research in HCI
- **作者：** Andy Crabtree
- **发布时间：** 2024-08-20

## 研究领域
- **其他**（人机交互 HCI / 社会计算 / 研究方法）

## 核心问题
这篇论文旨在解决人机交互（HCI）领域中一个普遍存在的方法论问题：评审者往往不恰当地使用**实证主义**的标准（如强调样本量、指标量化、测量统计）来评估**定性研究**。这种做法忽略了定性研究基于解释主义传统的本质（强调理解、阐释而非因果解释和量化），导致定性研究在评审中遭到不公正的拒绝或不当评价。论文试图通过阐述实证主义与解释主义的区别，提出一套适用于定性研究自身的评估标准，以解决这种“方法论上的不一致”。

## 主要创新点
1. **提出方法论反思的必要性**：论文批评了HCI领域普遍存在的“浅层”定性研究评审现状，指出这是一个系统性的方法论冲突，而非个别评审者的无知或运气不佳。它强调必须回到人文科学的基础来理解定性研究。
2. **建立了基于解释主义的评估框架**：不同于传统的实证科学评估指标，论文提出了五个基于解释主义传统的定性研究评审标准，指导评审者如何在不依赖数字和指标的情况下评估定性研究的质量。
3. **历史视角的论证路径**：通过回溯人文科学的起源（如涂尔干关于社会学的研究），论文从本体论、认识论和方法论层面厘清了自然科学与人文科学、实证主义与解释主义的根本区别，为定性研究提供了坚实的哲学合法性基础。

## 方法概述
这篇论文并非提出计算模型或算法架构，而是采用**社会学与历史分析**的方法。
1. **理论对比分析**：深入剖析实证主义与解释主义在对待“人类世界”时的根本差异（前者寻求因果律和量化，后者寻求历史和社会语境下的理解）。
2. **案例引证**：引用了作者自身关于AI风险研究的投稿被拒经历，以及Soden等人关于HCI社区对定性研究不当评审的调研数据，作为定性研究遭受实证主义标准误判的证据。
3. **标准构建**：基于人文科学的解释学传统，推导出五项具体的评审准则，用于替代量化的“KPI”指标。

## 实验结果
论文不涉及传统意义上的算法实验结果，但其论证过程展示了以下关键发现：
1. **定性研究的独特贡献**：定性研究的价值在于提供对人类行为和社会现象的深入理解，这种理解往往不能通过大样本的统计概括获得，而是通过揭示特定情境下的意义来实现。
2. **量化的局限性**：单纯追求样本量、代码本一致性或人口统计学分布，往往无法保证对复杂社会技术现象（如AI风险）的有效洞察。
3. **新标准的适用性**：论文提出的五项标准（如方法论的反思性、启发性概念的提供、分析的广度与效用等）能够帮助评审者识别出那些虽然样本量小但具有深刻洞见的优质研究。

## 局限性
1. **文化惯性的挑战**：论文指出，HCI领域深受自然科学训练影响，评审者往往无意识地（潜意识的偏见）应用实证主义逻辑。改变这种根深蒂固的评审文化不仅需要新的标准，还需要整个社区对人文科学哲学基础的再教育。
2. **实用性的权衡**：虽然作者承认定性研究在人文科学中不一定以“工具性”为目的，但在HCI领域，定性研究往往需要解决实际的技术设计问题。论文试图在“纯理解”与“实际效用”之间寻找平衡，但如何完全说服应用导向的评审者接受非量化的“质量保证”仍是一个长期的挑战。

</details>